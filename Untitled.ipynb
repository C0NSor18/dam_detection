{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from pprint import pprint\n",
    "from glob import glob\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, MaxPool2D, Input, Lambda, MaxPooling2D, UpSampling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import repeat\n",
    "from collections import Counter\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import re\n",
    "from scripts.constants import SEED\n",
    "from pprint import pprint\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_serialized_example(example_proto):\n",
    "    ''' Parser function\n",
    "    Useful for functional extraction, i.e. .map functions\n",
    "    \n",
    "    Args:\n",
    "        example_proto: a serialized example\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary with features, cast to float32\n",
    "        This returns a dictionary of keys and tensors to which I apply the transformations.\n",
    "    '''\n",
    "    # feature columns of interest\n",
    "    featuresDict = {\n",
    "        'B2': tf.io.FixedLenFeature([256, 256], dtype=tf.float32),  # B\n",
    "        'B3': tf.io.FixedLenFeature([256, 256], dtype=tf.float32),  # G\n",
    "        'B4': tf.io.FixedLenFeature([256, 256], dtype=tf.float32),  # R\n",
    "        #'AVE': tf.io.FixedLenFeature([256, 256], dtype=tf.float32), # Elevation\n",
    "        #'NDWI': tf.io.FixedLenFeature([256, 256], dtype=tf.float32), # vegetation index\n",
    "    }\n",
    "    \n",
    "    return tf.io.parse_single_example(example_proto, featuresDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def predict_input_fn(fileNames,side,bands):\n",
    "  \n",
    "    # Read `TFRecordDatasets` \n",
    "    dataset = tf.data.TFRecordDataset(fileNames, compression_type='GZIP')\n",
    "\n",
    "    featuresDict = {x:tf.io.FixedLenFeature([side, side], dtype=tf.float32) for x in bands}\n",
    "\n",
    "    # Make a parsing function\n",
    "    def parse_image(example_proto):\n",
    "        parsed_features = tf.io.parse_single_example(example_proto, featuresDict)\n",
    "        return parsed_features\n",
    "  \n",
    "    def stack_images(features):         \n",
    "        nfeat = tf.transpose(tf.squeeze(tf.stack(list(features.values()))))    \n",
    "        return nfeat\n",
    " \n",
    "    dataset = dataset.map(parse_image, num_parallel_calls=4)\n",
    "    dataset = dataset.map(stack_images, num_parallel_calls=4)   \n",
    "    dataset = dataset.batch(side*side)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = predict_input_fn('myExportImageTask-00000.tfrecord.gz', 256, ['B4', 'B3', 'B2'])\n",
    "\n",
    "#for x in dataset:\n",
    "#    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = iter(dataset)\n",
    "data = next(data)\n",
    "\n",
    "def stretch_image_colorspace(img):\n",
    "    max_val = np.max(img)\n",
    "    return img / max_val\n",
    "\n",
    "\n",
    "fullData = np.zeros([256*5, 256*3, 3])\n",
    "for n in range(0,3):\n",
    "    dataMat = np.zeros([256*5, 256*3])\n",
    "    z = 0\n",
    "    for j in range(0,3):\n",
    "        for i in range(0,5):\n",
    "            dataMat[i*256: (i+1)*256, j*256: (j+1)*256 ] = np.squeeze(data[z, :, :, n])\n",
    "            z += 1\n",
    "    fullData[:,:,n] = dataMat\n",
    "    \n",
    "fullData = stretch_image_colorspace(fullData)\n",
    "fullData = np.transpose(np.array([fullData[:,:,2], fullData[:,:,1], fullData[:,:,0]]), (1, 2 ,0 ))\n",
    "plt.imshow(np.fliplr(np.rot90(fullData, k=3)))\n",
    "print(np.max(fullData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataMat = np.resize(data, (256*5, 256*3))\n",
    "plt.imshow(dataMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_serialized_example(example_proto):\n",
    "    ''' Parser function\n",
    "    Useful for functional extraction, i.e. .map functions\n",
    "    \n",
    "    Args:\n",
    "        example_proto: a serialized example\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary with features, cast to float32\n",
    "        This returns a dictionary of keys and tensors to which I apply the transformations.\n",
    "    '''\n",
    "    # feature columns of interest\n",
    "    featuresDict = {\n",
    "        'B2': tf.io.FixedLenFeature([257, 257], dtype=tf.float32),  # B\n",
    "        'B3': tf.io.FixedLenFeature([257, 257], dtype=tf.float32),  # G\n",
    "        'B4': tf.io.FixedLenFeature([257, 257], dtype=tf.float32),  # R\n",
    "        'AVE': tf.io.FixedLenFeature([257, 257], dtype=tf.float32), # Elevation\n",
    "        'NDWI': tf.io.FixedLenFeature([257, 257], dtype=tf.float32), # vegetation index\n",
    "        'index': tf.io.FixedLenFeature([1], dtype=tf.int64), # index\n",
    "        'label': tf.io.FixedLenFeature([1], dtype=tf.float32) #label\n",
    "    }\n",
    "    \n",
    "    return tf.io.parse_single_example(example_proto, featuresDict)\n",
    "\n",
    "\n",
    "def stretch_image_colorspace(img):\n",
    "    max_val = tf.reduce_max(img)\n",
    "    return tf.cast(tf.divide(img, max_val), tf.float32)\n",
    "\n",
    "\n",
    "#using a closure so we can add extra params to the map function from tf.Dataset\n",
    "def parse_image(dims = [257, 257], channels = ['B4', 'B3', 'B2'], stretch_colorspace=True):\n",
    "    ''' Stack individual RGB bands into a N dimensional array\n",
    "    The RGB bands are still separate 1D arrays in the TFRecords, combine them into a single 3D array\n",
    "    \n",
    "    Args:\n",
    "        features: A dictionary with the features (RGB bands or other channels that need to be concatenated)\n",
    "    '''\n",
    "    \n",
    "    # print(\"using the general image parsing function\")\n",
    "    def parse_image_fun(features):\n",
    "        #channels = list(features.values())\n",
    "        label = features['label']\n",
    "        \n",
    "        # Get the image channels, and NDWI/AVE channels separately\n",
    "        # we cannot import them all at once since they need separate preprocessing steps\n",
    "        img_chan = [features[x] for x in channels if x in ['B4', 'B3', 'B2']]\n",
    "        ndwi_chan = [features[x] for x in channels if x in ['NDWI']]\n",
    "        ave_chan = [features[x] for x in channels if x in ['AVE']]\n",
    "    \n",
    "        \n",
    "        # stack the individual arrays, remove all redundant dimensions of size 1, and transpose them into the right order\n",
    "        # (batch size, H, W, channels)\n",
    "        img = tf.transpose(tf.squeeze(tf.stack(img_chan)))\n",
    "        \n",
    "        # stretch color spaces of the RGB channels\n",
    "        if stretch_colorspace:\n",
    "            img = stretch_image_colorspace(img)\n",
    "        \n",
    "        if ndwi_chan:\n",
    "            # further normalization?\n",
    "            img = tf.concat([img, tf.transpose(ndwi_chan)], axis= 2)\n",
    "        \n",
    "        if ave_chan:\n",
    "            # some kind of normalization needed?\n",
    "            img = tf.concat([img, tf.transpose(ave_chan)], axis= 2)\n",
    "        \n",
    "        # Additionally, resize the images to a desired size\n",
    "        img = tf.image.resize(img, dims)\n",
    "        return img, tf.reduce_max(tf.one_hot(tf.cast(label, dtype=tf.int32), 2, dtype=tf.int32), axis=0)\n",
    "    \n",
    "    return parse_image_fun\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'grand50.tfrecord.gz'\n",
    "\n",
    "dataset = tf.data.experimental.TFRecordDataset(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
