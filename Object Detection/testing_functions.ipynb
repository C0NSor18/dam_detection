{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data conversion script\n",
    "Convert data from TFRecord with nonstand data format to a standard one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../datasets/bbox_data/dam-data-000.gz', '../datasets/bbox_data/dam-data-001.gz', '../datasets/bbox_data/dam-data-002.gz', '../datasets/bbox_data/dam-data-003.gz', '../datasets/bbox_data/dam-data-004.gz', '../datasets/bbox_data/dam-data-005.gz', '../datasets/bbox_data/dam-data-006.gz', '../datasets/bbox_data/dam-data-007.gz', '../datasets/bbox_data/dam-data-008.gz', '../datasets/bbox_data/dam-data-009.gz', '../datasets/bbox_data/dam-data-010.gz', '../datasets/bbox_data/dam-data-011.gz', '../datasets/bbox_data/dam-data-012.gz', '../datasets/bbox_data/dam-data-013.gz', '../datasets/bbox_data/dam-data-014.gz', '../datasets/bbox_data/dam-data-015.gz', '../datasets/bbox_data/dam-data-016.gz', '../datasets/bbox_data/dam-data-017.gz', '../datasets/bbox_data/dam-data-018.gz', '../datasets/bbox_data/dam-data-019.gz', '../datasets/bbox_data/dam-data-020.gz', '../datasets/bbox_data/dam-data-021.gz', '../datasets/bbox_data/dam-data-022.gz', '../datasets/bbox_data/dam-data-023.gz', '../datasets/bbox_data/dam-data-024.gz', '../datasets/bbox_data/dam-data-025.gz', '../datasets/bbox_data/dam-data-026.gz']\n"
     ]
    }
   ],
   "source": [
    "input_path = '../datasets/bbox_data/*.gz'\n",
    "\n",
    "input_files= glob(input_path)\n",
    "#print(\"input path is \", input_files)\n",
    "print(input_files)\n",
    "data_path = '../datasets/raw/bbox_data.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_serialized_example(example_proto):\n",
    "    ''' Parser function\n",
    "    Useful for functional extraction, i.e. .map functions\n",
    "    \n",
    "    Args:\n",
    "        example_proto: a serialized example\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary with features, cast to float32\n",
    "        This returns a dictionary of keys and tensors to which I apply the transformations.\n",
    "    '''\n",
    "    # feature columns of interest\n",
    "    featuresDict = {\n",
    "        'image/height': tf.io.FixedLenFeature(1, dtype=tf.int64),\n",
    "        'image/width': tf.io.FixedLenFeature(1, dtype=tf.int64),\n",
    "        'image/filename': tf.io.FixedLenFeature(1, dtype=tf.string),\n",
    "        'image/source_id': tf.io.FixedLenFeature(1, dtype=tf.string),\n",
    "        'image/format': tf.io.FixedLenFeature(1, dtype=tf.string),\n",
    "        'image/channel/B2': tf.io.FixedLenFeature([257, 257], dtype=tf.float32),  # B\n",
    "        'image/channel/B3': tf.io.FixedLenFeature([257, 257], dtype=tf.float32),  # G\n",
    "        'image/channel/B4': tf.io.FixedLenFeature([257, 257], dtype=tf.float32),  # R\n",
    "        'image/channel/AVE': tf.io.FixedLenFeature([257, 257], dtype=tf.float32), # Elevation\n",
    "        'image/channel/NDWI': tf.io.FixedLenFeature([257, 257], dtype=tf.float32), # water index\n",
    "        'image/channel/MNDWI': tf.io.FixedLenFeature([257, 257], dtype=tf.float32), # water index\n",
    "        'image/channel/AWEINSH': tf.io.FixedLenFeature([257, 257], dtype=tf.float32), # water index\n",
    "        'image/channel/AWEISH': tf.io.FixedLenFeature([257, 257], dtype=tf.float32), # water index\n",
    "        'image/object/bbox/xmin': tf.io.FixedLenFeature(1, dtype=tf.int64), \n",
    "        'image/object/bbox/xmax': tf.io.FixedLenFeature(1, dtype=tf.int64),\n",
    "        'image/object/bbox/ymin': tf.io.FixedLenFeature(1, dtype=tf.int64),\n",
    "        'image/object/bbox/ymax': tf.io.FixedLenFeature(1, dtype=tf.int64),\n",
    "        'image/object/class/text': tf.io.FixedLenFeature(1, dtype=tf.string),\n",
    "        'image/object/class/label': tf.io.FixedLenFeature(1, dtype=tf.int64)\n",
    "    }\n",
    "    \n",
    "    return tf.io.parse_single_example(example_proto, featuresDict)\n",
    "\n",
    "def parse_features(feature):\n",
    "    features = [feature[x] for x in ['image/width', 'image/height', 'image/filename', 'image/object/class/label']]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: id=1404, shape=(1,), dtype=int64, numpy=array([257])>, <tf.Tensor: id=1405, shape=(1,), dtype=int64, numpy=array([257])>, <tf.Tensor: id=1406, shape=(1,), dtype=string, numpy=array([b'dam-0'], dtype=object)>, <tf.Tensor: id=1407, shape=(1,), dtype=int64, numpy=array([1])>)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.TFRecordDataset(input_files[0], compression_type='GZIP')\n",
    "dataset = dataset.map(parse_serialized_example)\n",
    "dataset = dataset.map(parse_features)\n",
    "\n",
    "\n",
    "for x in dataset.take(1):\n",
    "    print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
